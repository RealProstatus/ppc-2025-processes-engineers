# Линейная фильтрация изображений (блочное разбиение). Ядро Гаусса 3x3.

- Студент: Мельник Иван Вадимович, группа 3823Б1ПР1
- Технологии: SEQ, MPI
- Вариант: 28

## 1. Введение
В работе рассмотрена задача линейной фильтрации (размытия) изображения с использованием ядра Гаусса 3×3. Цель работы — разработать последовательную и MPI-параллельную версии алгоритма, сравнить их производительность и проанализировать влияние коммуникаций на масштабируемость.

## 2. Постановка задачи
**Формальная постановка**\
Дано изображение размером `H × W`, заданное в виде плоского массива пикселей `data` в порядке строк: `data[y * W + x]`. Требуется получить изображение того же размера, применив к каждому пикселю свёртку с ядром Гаусса 3×3:
\[
K=\begin{pmatrix}
1 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 1
\end{pmatrix}, \quad \sum K = 16
\]

**Дополнительные требования и ограничения**
- Алгоритмы должны корректно работать для любых размеров изображения, включая случаи, когда размеры не делятся нацело на количество процессов;
- Параллельная версия должна быть реализована с помощью MPI и быть стабильной на любом разумном количестве процессов;
- Полным входным изображением владеет только нулевой процесс; остальные процессы получают лишь свои блоки данных;
- Граничные пиксели обрабатываются с ограничением: выход за границу изображения заменяется ближайшим допустимым пикселем;
- Для уменьшения коммуникаций данные изображения передаются в формате `uint8_t` (1 байт на пиксель).

## 3. Базовый алгоритм (последовательная версия)
**Принцип работы**\
Последовательный алгоритм для каждого пикселя `(x, y)` вычисляет взвешенную сумму значений из его 3×3 окрестности и нормирует результат делением на 16. Для граничных пикселей используется clamp: координаты соседних пикселей, выходящие за пределы `[0..W-1]` и `[0..H-1]`, заменяются ближайшими допустимыми координатами.

**Структура циклов**\
Внешние циклы перебирают все пиксели изображения по координатам `y = 0..H-1` и `x = 0..W-1`. Внутренние циклы перебирают элементы ядра `dy = -1..1`, `dx = -1..1`.

**Реализация**\
Пиксель из окрестности читается как `data[clamp(y+dy)*W + clamp(x+dx)]`. Сумма накапливается в целочисленной переменной `acc` (тип `int`), чтобы исключить переполнение при сложении. Итоговое значение вычисляется как `(acc + 8) / 16`, где `+8` реализует округление к ближайшему целому при делении на 16. Результат записывается в выходной массив `out[y*W + x]`.

**Вычислительная сложность**\
На каждый пиксель выполняется 9 умножений и 8 сложений, поэтому временная сложность равна `O(W·H)`. Пространственная сложность — `O(W·H)` на хранение выходного изображения (входной массив не изменяется).

## 4. Схема распараллеливания
### 4.1. Общая идея
Изображение разбивается на прямоугольные блоки между MPI-процессами. Каждый процесс получает свой блок пикселей и вычисляет свёртку только для пикселей своего блока. Для корректного вычисления значений на границах блоков выполняется обмен граничными пикселями (halo exchange) с соседними процессами.

### 4.2. Распределение данных
**Топология процессов**\
Для `P` процессов выбирается прямоугольная сетка `P_r × P_c`, где `P_r · P_c = P`. Процесс с `rank` имеет координаты в сетке:
- `pr = rank / P_c`,
- `pc = rank % P_c`.
Сетка подбирается среди делителей `P` так, чтобы соотношение `P_r/P_c` было близко к аспекту изображения `H/W` (это уменьшает объём обмена граничными пикселями).

**Распределение остатка**\
Разбиение выполняется отдельно по ширине и высоте, с распределением остатка по первым блокам:
- `base_w = W / P_c`, `rem_w = W % P_c`,
- `base_h = H / P_r`, `rem_h = H % P_r`.\
Тогда размеры и смещения блока для координат `(pr, pc)`:
- `local_w = base_w + (pc < rem_w ? 1 : 0)`,
- `start_x = pc * base_w + min(pc, rem_w)`,
- `local_h = base_h + (pr < rem_h ? 1 : 0)`,
- `start_y = pr * base_h + min(pr, rem_h)`.
Если `local_w == 0` или `local_h == 0`, процесс получает пустой блок и проходит все стадии без вычислений.

**Передача данных блока**\
Нулевой процесс отправляет каждому процессу его прямоугольный блок входного изображения. Для исключения лишнего копирования используется MPI-тип подмассива (`MPI_Type_create_subarray`), позволяющий отправлять "подпрямоугольник" без предварительной упаковки в отдельный буфер. Получатель принимает блок в плотный локальный буфер размера `local_w * local_h`.
Для пересылки байтов пикселей используется тип `MPI_BYTE`, а обмен блоками выполняется через блокирующие `MPI_Send/MPI_Recv`.

### 4.3. Локальные вычисления
**Расширенный локальный буфер (halo)**\
Для свёртки 3×3 процессу требуется окрестность радиуса 1 вокруг блока. Поэтому локальный блок расширяется до массива размера `(local_h + 2) × (local_w + 2)`, где внутренняя область `[1..local_h]×[1..local_w]` содержит исходные данные блока, а границы — получены из обмена между процессами (halo).

**Заполнение halo**\
Обмен выполняется в трёх частях:
- строки halo (верхняя и нижняя) — с соседями `up/down`;
- столбцы halo (левый и правый) — с соседями `left/right`;
- четыре угловых пикселя — с диагональными соседями `up_left`, `up_right`, `down_left`, `down_right`.

Если сосед отсутствует (процесс находится на границе изображения или у соседа пустой блок), соответствующая halo-зона остаётся заполненной по правилу clamp (копированием собственных граничных пикселей). После обмена дополнительно корректируются углы halo, чтобы они соответствовали уже обновлённым граничным строкам/столбцам (это важно для корректной свёртки на стыках блоков при отсутствии диагонального соседа).

**Вычисление свёртки**\
После построения расширенного буфера процесс вычисляет выходные значения для пикселей своего блока. Для каждого локального `(x, y)` сумма берётся из окна 3×3 в расширенном буфере, после чего применяется нормировка `(acc + 8)/16`. Результат записывается в локальный выходной буфер `local_out` размера `local_w*local_h`.

### 4.4. Сбор результатов
После локальных вычислений нулевой процесс собирает блоки результата от всех процессов в общий выходной массив размером `W*H`. Для сборки также используется `MPI_Type_create_subarray`, что позволяет принимать локальные блоки сразу в нужные позиции глобального массива.

**Примечание про режимы тестирования**\
В функциональных тестах итоговое изображение дополнительно может быть распространено всем процессам, чтобы обеспечить одинаковую проверку на каждом ранге. В тестах производительности полный результат сохраняется только на процессе 0, чтобы не тратить время на пересылку всего изображения между процессами.

### 4.5. Синхронизация процессов
Все процессы выполняют одинаковую последовательность стадий: получение размеров, распределение блока, halo-обмен, локальная свёртка и отправка результата. Нулевой процесс дополнительно выполняет распределение входных данных и сборку результата.

## 5. Детали реализации
### 5.1. Структура реализации
```text
melnik_i_gauss_block_part
│   settings.json
│   report.md
│
├───common
│   └───include
│           common.hpp - определение типов входных/выходных/тестовых данных
├───mpi
│   ├───include
│   │       ops_mpi.hpp - заголовочный файл MPI-реализации
│   │
│   └───src
│           ops_mpi.cpp - код MPI-реализации
├───seq
│   ├───include
│   │       ops_seq.hpp - заголовочный файл SEQ-реализации
│   │
│   └───src
│           ops_seq.cpp - код SEQ-реализации
└───tests
    ├───functional
    │       main.cpp - функциональные тесты
    │
    └───performance
            main.cpp - тесты производительности
```

### 5.2. Основные классы / функции
- `MelnikIGaussBlockPartSEQ` — последовательная реализация фильтра Гаусса 3×3.
- `MelnikIGaussBlockPartMPI` — MPI-реализация:
  - `ComputeProcessGrid` — выбор сетки процессов `P_r × P_c`;
  - `ComputeBlockInfo` / `ComputeBlockInfoByCoords` — вычисление координат и размеров блока процесса;
  - `FillExtendedWithClamp` — построение расширенного буфера блока с начальным заполнением по правилу clamp;
  - `ExchangeHalos` — halo-обмен с соседями (строки/столбцы/углы), реализованный через вспомогательные функции:
    - `ComputeNeighbours`, `ExchangeRowHalos`, `ExchangeColHalos`, `ExchangeCornerHalos`, `FixCornersWithoutDiagonal`;
  - `ApplyGaussianFromExtended` — вычисление свёртки по расширенному буферу.

### 5.3. Обработка граничных случаев и замечания
- Процессы, получающие пустой блок (`local_w==0` или `local_h==0`), корректно завершают выполнение, участвуя в коммуникациях с нулевыми объёмами данных.
- На границах изображения используется clamp-обработка, эквивалентная последовательной версии.
- Для корректности на стыках блоков реализован halo-обмен, включая углы, а также корректировка угловых halo-значений при отсутствии диагонального соседа.

### 5.4. Пространственная и временная сложности алгоритмов
Для изображения `W × H`:
- **SEQ**: время `O(W·H)`, память `O(W·H)` на выход.
- **MPI**: время примерно `O((W·H)/P + x)`, где `x` - рассылка блоков, halo-обмен и сбор результата. Память на рабочем процессе — `O((W·H)/P)`.

## 6. Тестовая инфраструктура
### 6.1. Аппаратное обеспечение
| Параметр | Значение |
| -------- | -------- |
| CPU      | (заполнить) |
| RAM      | (заполнить) |

### 6.2. Программное обеспечение:
| Параметр   | Значение |
| ---------- | -------- |
| ОС         | (заполнить) |
| MPI        | (заполнить) |
| Компилятор | (заполнить) |
| Сборка     | Release |

### 6.3. Тестовые данные
- **Функциональные тесты:** набор малых предсказуемых изображений (квадратных и прямоугольных), а также тонкие случаи `1×N` и `N×1`. Корректность проверяется сравнением с эталонной последовательной свёрткой.
- **Perf-тест:** обработка реального изображения большого размера (например 8K), данные загружаются на нулевом процессе.

## 7. Результаты и обсуждение
### 7.1. Корректность
Функциональные тесты подтверждают совпадение результатов SEQ и MPI реализаций на разных количествах процессов, включая случаи с прямоугольными изображениями и неделимостью по блокам.

### 7.2. Производительность
**Метрики:**
1. Абсолютное время выполнения вычислительной части алгоритма в миллисекундах;
2. Ускорение относительно последовательной версии;
3. Эффективность распараллеливания = `(ускорение / число процессов) * 100%`.

**Тестовое изображение = (указать размер, например 7680×4320)**

| Режим | Процессы | Время, мс | Ускорение | Эффективность |
| ----- | -------- | --------- | --------- | ------------- |
| seq   | 1        |           | 1.000     | N/A           |
| mpi   | 2        |           |           |               |
| mpi   | 4        |           |           |               |
| mpi   | 7        |           |           |               |
| mpi   | 8        |           |           |               |

**Анализ полученных результатов:**\
(заполнить после получения измерений; рекомендуется обсудить вклад halo-обмена и сборки результата, а также влияние формы разбиения на масштабируемость).

## 8. Выводы
Реализованы устойчивые последовательная и MPI-версии фильтра Гаусса 3×3 для изображений произвольного размера. MPI-реализация использует 2D-блочную декомпозицию с распределением остатка, halo-обмен для корректной обработки границ блоков и сбор результата на нулевом процессе. Работа демонстрирует влияние коммуникаций и формы разбиения на общую производительность.

## 9. Источники
1. Документация по курсу «Параллельное программирование» // Parallel Programming Course URL: https://learning-process.github.io/parallel_programming_course/ru/index.html (дата обращения: 10.11.2025).
2. Gaussian blur (свёртка и ядра фильтров) // Cppreference и материалы по обработке изображений (дата обращения: 22.12.2025).
3. Open MPI documentation // Open MPI (дата обращения: 22.12.2025).

