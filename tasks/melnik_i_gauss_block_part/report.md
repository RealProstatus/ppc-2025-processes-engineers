# Линейная фильтрация изображений (блочное разбиение). Ядро Гаусса 3x3.

- Студент: Мельник Иван Вадимович, группа 3823Б1ПР1
- Технологии: SEQ, MPI
- Вариант: 28

## 1. Введение
В работе рассмотрена задача линейной фильтрации (размытия) изображения с использованием ядра Гаусса 3×3. Цель работы — разработать последовательную и MPI-параллельную версии алгоритма, сравнить их производительность и проанализировать влияние коммуникаций на масштабируемость.

## 2. Постановка задачи
**Формальная постановка**\
Дано изображение размером `H × W`, заданное в виде плоского массива пикселей `data` в порядке строк: `data[y * W + x]`. Требуется получить изображение того же размера, применив к каждому пикселю свёртку с ядром Гаусса 3×3:
\[
K=\begin{pmatrix}
1 & 2 & 1 \\
2 & 4 & 2 \\
1 & 2 & 1
\end{pmatrix}, \quad \sum K = 16
\]

**Дополнительные требования и ограничения**
- Алгоритмы должны корректно работать для любых размеров изображения, включая случаи, когда размеры не делятся нацело на количество процессов;
- Параллельная версия должна быть реализована с помощью MPI и быть стабильной на любом разумном количестве процессов;
- Полным входным изображением владеет только нулевой процесс; остальные процессы получают лишь свои блоки данных;
- Граничные пиксели обрабатываются с ограничением: выход за границу изображения заменяется ближайшим допустимым пикселем;
- Для уменьшения коммуникаций данные изображения передаются в формате `uint8_t` (1 байт на пиксель).

## 3. Базовый алгоритм (последовательная версия)
**Принцип работы**\
Последовательный алгоритм для каждого пикселя `(x, y)` вычисляет взвешенную сумму значений из его 3×3 окрестности и нормирует результат делением на 16. Для граничных пикселей используется clamp: координаты соседних пикселей, выходящие за пределы `[0..W-1]` и `[0..H-1]`, заменяются ближайшими допустимыми координатами.

**Структура циклов**\
Внешние циклы перебирают все пиксели изображения по координатам `y = 0..H-1` и `x = 0..W-1`. Внутренние циклы перебирают элементы ядра `dy = -1..1`, `dx = -1..1`.

**Реализация**\
Пиксель из окрестности читается как `data[clamp(y+dy)*W + clamp(x+dx)]`. Сумма накапливается в целочисленной переменной `acc` (тип `int`), чтобы исключить переполнение при сложении. Итоговое значение вычисляется как `(acc + 8) / 16`, где `+8` реализует округление к ближайшему целому при делении на 16. Результат записывается в выходной массив `out[y*W + x]`.

**Вычислительная сложность**\
На каждый пиксель выполняется 9 умножений и 8 сложений, поэтому временная сложность равна `O(W·H)`. Пространственная сложность — `O(W·H)` на хранение выходного изображения (входной массив не изменяется).

## 4. Схема распараллеливания
### 4.1. Общая идея
Изображение разбивается на прямоугольные блоки между MPI-процессами. Каждый процесс получает свой блок пикселей и вычисляет свёртку только для пикселей своего блока. Для корректного вычисления значений на границах блоков выполняется обмен граничными пикселями (halo exchange) с соседними процессами.

### 4.2. Распределение данных
**Топология процессов**\
Для `P` процессов выбирается прямоугольная сетка `P_r × P_c`, где `P_r · P_c = P`. Процесс с `rank` имеет координаты в сетке:
- `pr = rank / P_c`,
- `pc = rank % P_c`.
Сетка подбирается среди делителей `P` так, чтобы соотношение `P_r/P_c` было близко к аспекту изображения `H/W` (это уменьшает объём обмена граничными пикселями).

**Распределение остатка**\
Разбиение выполняется отдельно по ширине и высоте, с распределением остатка по первым блокам:
- `base_w = W / P_c`, `rem_w = W % P_c`,
- `base_h = H / P_r`, `rem_h = H % P_r`.\
Тогда размеры и смещения блока для координат `(pr, pc)`:
- `local_w = base_w + (pc < rem_w ? 1 : 0)`,
- `start_x = (pc * base_w) + min(pc, rem_w)`,
- `local_h = base_h + (pr < rem_h ? 1 : 0)`,
- `start_y = (pr * base_h) + min(pr, rem_h)`.
Если `local_w == 0` или `local_h == 0`, процесс получает пустой блок и проходит все стадии без вычислений.

**Передача данных блока**\
Нулевой процесс отправляет каждому процессу его прямоугольный блок входного изображения. Для исключения лишнего копирования используется MPI-тип подмассива (`MPI_Type_create_subarray`), позволяющий отправлять "подпрямоугольник" без предварительной упаковки в отдельный буфер. Получатель принимает блок в плотный локальный буфер размера `local_w * local_h`.
Для пересылки байтов пикселей используется тип `MPI_BYTE`, а обмен блоками выполняется через блокирующие `MPI_Send/MPI_Recv`.

### 4.3. Локальные вычисления
**Расширенный локальный буфер (halo)**\
Для свёртки 3×3 процессу требуется окрестность радиуса 1 вокруг блока. Поэтому локальный блок расширяется до массива размера `(local_h + 2) × (local_w + 2)`, где внутренняя область `[1..local_h]×[1..local_w]` содержит исходные данные блока, а границы — получены из обмена между процессами (halo).

**Заполнение halo**\
Обмен выполняется в трёх частях с использованием `MPI_Sendrecv` для одновременной отправки и приёма данных:
- строки halo (верхняя и нижняя) — с соседями `up/down`;
- столбцы halo (левый и правый) — с соседями `left/right`;
- четыре угловых пикселя — с диагональными соседями `up_left`, `up_right`, `down_left`, `down_right`.

Если сосед отсутствует (процесс находится на границе изображения или у соседа пустой блок), соответствующая halo-зона остаётся заполненной по правилу clamp (копированием собственных граничных пикселей). После обмена дополнительно корректируются углы halo, чтобы они соответствовали уже обновлённым граничным строкам/столбцам (это важно для корректной свёртки на стыках блоков при отсутствии диагонального соседа).

**Вычисление свёртки**\
После построения расширенного буфера процесс вычисляет выходные значения для пикселей своего блока. Для каждого локального `(x, y)` сумма берётся из окна 3×3 в расширенном буфере, после чего применяется нормировка `(acc + 8)/16`. Результат записывается в локальный выходной буфер `local_out` размера `local_w*local_h`.

### 4.4. Сбор результатов
После локальных вычислений нулевой процесс собирает блоки результата от всех процессов в общий выходной массив размером `W*H`. Для сборки также используется `MPI_Type_create_subarray`, что позволяет принимать локальные блоки сразу в нужные позиции глобального массива.

**Примечание про режимы тестирования**\
В функциональных тестах итоговое изображение дополнительно может быть распространено всем процессам, чтобы обеспечить одинаковую проверку на каждом ранге. В тестах производительности полный результат сохраняется только на процессе 0, чтобы не тратить время на пересылку всего изображения между процессами.

### 4.5. Синхронизация процессов
Все процессы выполняют одинаковую последовательность стадий: получение размеров, распределение блока, halo-обмен, локальная свёртка и отправка результата. Нулевой процесс дополнительно выполняет распределение входных данных и сборку результата.

## 5. Детали реализации
### 5.1. Структура реализации
```text
melnik_i_gauss_block_part
│
├───common
│   └───include
│           common.hpp - определение типов входных/выходных/тестовых данных
├───mpi
│   ├───include
│   │       ops_mpi.hpp - заголовочный файл MPI-реализации
│   │
│   └───src
│           ops_mpi.cpp - код MPI-реализации
├───seq
│   ├───include
│   │       ops_seq.hpp - заголовочный файл SEQ-реализации
│   │
│   └───src
│           ops_seq.cpp - код SEQ-реализации
└───tests
    ├───functional
    │       main.cpp - функциональные тесты
    │
    └───performance
            main.cpp - тесты производительности
```

### 5.2. Основные классы / функции
- `MelnikIGaussBlockPartSEQ` — последовательная реализация фильтра Гаусса 3×3.
- `MelnikIGaussBlockPartMPI` — MPI-реализация:
  - `ComputeProcessGrid` — выбор сетки процессов `P_r × P_c`;
  - `ComputeBlockInfo` / `ComputeBlockInfoByCoords` — вычисление координат и размеров блока процесса;
  - `FillExtendedWithClamp` — построение расширенного буфера блока с начальным заполнением по правилу clamp;
  - `ExchangeHalos` — halo-обмен с соседями (строки/столбцы/углы), реализованный через вспомогательные функции:
    - `ComputeNeighbours`, `ExchangeRowHalos`, `ExchangeColHalos`, `ExchangeCornerHalos`, `FixCornersWithoutDiagonal`;
  - `ApplyGaussianFromExtended` — вычисление свёртки по расширенному буферу;
  - `BroadcastImageSize` — рассылка размеров изображения всем процессам;
  - `BuildAllBlocks` — построение информации о блоках для всех процессов;
  - `SendBlocksToOthers` — отправка блоков данных от корневого процесса остальным;
  - `ScatterBlock` — распределение блоков входного изображения;
  - `ComputeLocal` — локальное вычисление свёртки с halo-обменом;
  - `GatherGlobal` — сбор результатов от всех процессов на корневой;
  - `FinalizeOutput` — финализация результата.

### 5.3. Обработка граничных случаев и замечания
- Процессы, получающие пустой блок (`local_w==0` или `local_h==0`), корректно завершают выполнение, участвуя в коммуникациях с нулевыми объёмами данных.
- На границах изображения используется clamp-обработка, эквивалентная последовательной версии.
- Для корректности на стыках блоков реализован halo-обмен, включая углы, а также корректировка угловых halo-значений при отсутствии диагонального соседа.

### 5.4. Пространственная и временная сложности алгоритмов
Для изображения `W × H`:
- **SEQ**: время `O(W·H)`, память `O(W·H)` на выход.
- **MPI**: время примерно `O((W·H)/P + x)`, где `x` - рассылка блоков, halo-обмен и сбор результата. Память на рабочем процессе — `O((W·H)/P)`.

## 6. Тестовая инфраструктура
### 6.1. Аппаратное обеспечение
| Параметр | Значение                                                                               |
| -------- | -------------------------------------------------------------------------------------- |
| CPU      | Intel Core i5-11400F (6 cores, 12 threads, 2.6 GHz, up to 4.40 Ghz, L3 Cache 12 MB)    |
| RAM      | 32 GB DDR4 (3200 MHz)                                                                  |
### 6.2. Программное обеспечение:
| Параметр   | Значение                                               |
| ---------- | ------------------------------------------------------ |
| ОС         | Windows 10 Pro 22H2 (19045.5965) + WSL (Ubuntu 13.3.0) |
| MPI        | OpenMPI 3.1                                            |
| Компилятор | g++ 14.2.0                                             |
| Сборка     | Release                                                |

### 6.3. Тестовые данные
- **Функциональные тесты:** набор малых предсказуемых изображений (квадратных и прямоугольных), а также случаи `1×N` и `N×1`. Корректность проверяется сравнением с эталонной последовательной свёрткой.
- **Perf-тест:** обработка реального изображения большого размера 14694x8266 пикселей (`moon.jpg`), данные загружаются на нулевом процессе. Для стабильности прохождения CI используется изображение меньшего размера 8192x8192 пикселей (`verybig.jpg`).

## 7. Результаты и обсуждение
### 7.1. Корректность
Функциональные тесты подтверждают совпадение результатов SEQ и MPI реализаций на разных количествах процессов, включая случаи с прямоугольными изображениями и неделимостью по блокам.

### 7.2. Производительность
**Метрики:**
1. Абсолютное время выполнения вычислительной части алгоритма в миллисекундах;
2. Ускорение относительно последовательной версии;
3. Эффективность распараллеливания = `(ускорение / число процессов) * 100%`.

**Тестовое изображение = 14694x8266**

| Режим | Процессы | Время, мс | Ускорение | Эффективность |
| ----- | -------- | --------- | --------- | ------------- |
| seq   | 1        | 4594      | 1.000     | N/A           |
| mpi   | 2        | 2654      | 1.730     | 86,54%        |
| mpi   | 4        | 2036      | 2.256     | 56,40%        |
| mpi   | 7        | 2046      | 2.245     | 32,07%        |
| mpi   | 8        | 1876      | 2.448     | 30,61%        |
| mpi   | 32       | 4172      | 1.101     | 03.44%        |

**Анализ полученных результатов:**

При малом числе процессов параллелизация эффективна. Для 2 процессов эффективность достигает 86.54%, что близко к линейному масштабированию. Это объясняется малыми накладными расходами на коммуникации: объем данных на процесс достаточно велик (~14694×8266/2 ≈ 60 млн пикселей), а периметр блока относительно мал. Для 4 процессов эффективность снижается до 56.40%, но время выполнения продолжает улучшаться (ускорение 2.256). Это указывает на то, что вычислительные выгоды от параллелизма пока перевешивают рост коммуникационных затрат.\
Время выполнения при 7 процессах (2046 мс) практически идентично времени при 4 процессах (2036 мс), при этом эффективность падает до 32.07%. Основная причина — неоптимальное разбиение: число 7 является простым, поэтому алгоритм выбора сетки процессов (`ComputeProcessGrid`) не может построить близкую к квадратной 2D-решетку. Выбирается сетка 7×1 или 1×7, что приводит к резкому росту объема halo-обмена. При этом размер локального блока на процесс становится слишком малым, как следствие, коммуникационные затраты начинают доминировать над вычислительными.
Несмотря на дальнейшее снижение эффективности до 30.61%, время выполнения улучшается (1876 мс, ускорение 2.448) по сравнению с 7 процессами. Это объясняется тем, что 8 = 2×4 или 4×2, что позволяет построить чуть более сбалансированную 2D-сетку с меньшим числом halo-обменов, несмотря на увеличение числа процессов.\
При 32 процессах эффективность — 3.44%. Это объясняется тем, что значительно возросло количество коммуникационных операций (scatter, halo-обмены, gather), они начинают доминировать над локальными вычислениями. Дополнительно, при большом числе процессов возрастают задержки синхронизации процессов.

**Общий вывод по результатам тестирования:** Для данного размера изображения (14694×8266) оптимальное число процессов находится в диапазоне 4–8. Дальнейшее увеличение числа процессов приводит к доминированию коммуникационных затрат, особенно halo-обмена. К тому же выяснилось, что данный алгоритм выбирает неоптимальное блочное разбиение при количестве процессов, равном простому числу

## 8. Выводы
Реализованы устойчивые последовательная и MPI-версии фильтра Гаусса 3×3 для изображений произвольного размера. MPI-реализация использует 2D-блочную декомпозицию с распределением остатка, halo-обмен для корректной обработки границ блоков и сбор результата на нулевом процессе. Работа демонстрирует влияние коммуникаций и формы разбиения на общую производительность.

## 9. Источники
1. Документация по курсу «Параллельное программирование» // Parallel Programming Course URL: https://learning-process.github.io/parallel_programming_course/ru/index.html (дата обращения: 10.11.2025).
2. Фильтр Гаусса на стероидах: секреты ускорения вычислений // Хабр URL: https://habr.com/ru/companies/smartengines/articles/877082/ (дата обращения: 20.12.2025).
3. Intel® Core™ i5-11400F Processor // Intel® Products URL: https://www.intel.com/content/www/us/en/products/sku/212271/intel-core-i511400f-processor-12m-cache-up-to-4-40-ghz/specifications.html (дата обращения: 18.11.2025).
4. Open MPI v3.1.6 documentation // Open MPI URL: https://www.open-mpi.org/doc/v3.1/ (дата обращения: 12.11.2025).

