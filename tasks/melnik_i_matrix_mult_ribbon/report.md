# Ленточное умножение матриц (разбиение по строкам A)

- Студент: Мельник Иван Вадимович, группа 3823Б1ПР1
- Технологии: SEQ, MPI
- Вариант: 2

## 1. Введение
В работе рассмотрена задача умножения квадратных матриц с ленточным (по строкам) разбиением матрицы `A` между MPI-процессами. Цель — получить корректные и устойчивые к санитайзерам реализации (SEQ и MPI), способные работать при произвольном разумном числе процессов, включая случаи `n < p`, а также обеспечить кроссплатформенную сборку и воспроизводимость.

## 2. Постановка задачи
**Формальная постановка**\
Даны матрицы `A (n × n)` и `B (n × n)`. Требуется вычислить `C = A · B`.\
**Дополнительные требования и ограничения**
- Поддержка неделимости `n` на число процессов: строки `A` распределяются с остатком.
- Корректная работа на 1/2/4/6 процессах; процессы без строк обязаны завершать работу без сбоев.
- Последовательная версия — классический трёхцикловый алгоритм.
- Параллельная версия — ленточное разбиение `A`, широковещание транспонированной `B`, сбор результата на root.

## 3. Базовый алгоритм (последовательная версия)
**Идея**\
Прямое умножение `C[i][j] = Σ_k A[i][k] · B[k][j]` с предвалидацией входных матриц на прямоугольность и согласованность.\
**Этапы алгоритма**
1. Проверить, что все строки `A` и `B` имеют одинаковую ширину, и что `cols(A) == rows(B)`.
2. Выделить `C` размера `rows(A) × cols(B)` и обнулить.
3. Три вложенных цикла (`i`, `k`, `j`), накапливая произведения.

## 4. Схема распараллеливания
### 4.1. Общая идея
Строки `A` делятся между процессами с учётом остатка; `B` заранее транспонируется и рассылается всем. Каждый процесс умножает свой блок строк на общую `B^T` и возвращает результат для последующей сборки.
### 4.1. Алгоритм параллельного выполнения
1. Инициализация: определение `proc_rank/proc_num` в `MPI_COMM_WORLD`.
2. Валидация на root: проверка прямоугольности, ненулевых размеров, условия `cols_a == rows_b`.
3. Рассылка размеров (`MPI_Bcast`) и плоского буфера `B^T`.
4. Распределение строк `A` с остатком: вычисление `rows_per_rank`, подготовка `counts/displs`, передача через `MPI_Scatterv` плоского `A`.
5. Локальные вычисления: для каждой локальной строки — свёртка с соответствующим столбцом `B` через `std::transform_reduce`.
6. Сбор результата: `MPI_Gatherv` в плоский `C` на root с последующей рассылкой всем для единообразного постобработчика.
### 4.2. Топология процессов
Все процессы равноправны в вычислениях; root дополнительно агрегирует и распространяет итоговую матрицу.

## 5. Детали реализации
### 5.1. Структура реализации
```text
melnik_i_matrix_mult_ribbon
    ├── common/include/common.hpp    # типы In/Out/Test
    ├── seq/include|src              # последовательная реализация
    ├── mpi/include|src              # MPI-реализация (ленточное A)
    └── tests/functional|performance # gtest: корректность и производительность
```
### 5.2. Основные классы / функции
- `MelnikIMatrixMultRibbonSEQ` — последовательное умножение.
- `MelnikIMatrixMultRibbonMPI`:
  - `ValidateOnRoot`, `HasUniformRowWidth` — строгая валидация входа;
  - `ShareSizes`, `ShareMatrixB` — широковещание метаданных и `B^T`;
  - `ScatterRows` — распределение строк `A` с остатком;
  - `MultiplyLocal` — локальное умножение с `transform_reduce`;
  - `GatherAndDistribute` — сбор плоского `C` на root и рассылка всем.
### 5.3. Обработка граничных случаев и замечания
- Процессы, получающие 0 строк, проходят все стадии без вычислений.
- Валидация отклоняет рваные матрицы и несовпадающие размеры.
- Постобработка преобразует плоский буфер `C` в вектор векторов.
### 5.4. Пространственная и временная сложности алгоритмов
- SEQ: `O(n^3)` по времени, `O(n^2)` по памяти.
- MPI: примерно `O((n^3)/p + n^2)` (умножение + широковещание/сбор), память — `O(n^2/p)` на рабочих рангах и `O(n^2)` на root.

## 6. Тестовая инфраструктура
### 6.1. Аппаратное обеспечение
Ориентация на CI-пулы с поддержкой 1–6 MPI-процессов; специальных аппаратных привязок не задавалось.
### 6.2. Программное обеспечение
- ОС: Ubuntu (CI) и Windows/WSL локально.
- Компиляторы: gcc/clang/MSVC; санитайзеры включаются в конфигурациях CI.
- MPI: OpenMPI.
### 6.3. Тестовые данные
- **Функциональные тесты (9 кейсов, 6×6 и мини-варианты 1×1, 1×3·3×1):**
  - Единичные, нулевые, диагональные (в т.ч. с крупными и отрицательными значениями);
  - Полностью единичные матрицы (ожидаемые элементы равны 6);
  - Инкрементные строки * I (проверка тождественности);
  - Альтернирующие знаки * диагональ;
  - Векторные случаи 1×3·3×1 и скаляр 1×1.
- **Perf-тест:** умножение плотных матриц 800×800; критерий — непустой результат.

## 7. Результаты и обсуждение
### 7.1. Корректность
Функциональные тесты охватывают разнообразные граничные и типовые сценарии. SEQ и MPI выдают идентичные результаты на наборах 1/2/4/6 процессов, включая случаи `n < p`, что подтверждает корректность распределения с остатком и сборки.
### 7.2. Производительность
Специальные численные замеры в отчёт не включены: perf-тест выполняет нагрузку 800×800 и служит индикатором стабильности. Ожидается, что ускорение зависит от конфигурации раннеров; характерные коммуникационные затраты (Bcast + Gatherv) ограничивают масштабирование при малых `n`, но при росте размера отношение вычислений к коммуникациям улучшается.

## 8. Выводы
Получены устойчивые SEQ и MPI реализации ленточного умножения квадратных матриц с учётом разбиения с остатком и строгой валидации входных данных. Тестовый набор покрывает граничные и векторные случаи, а perf-тест подтверждает работоспособность на средних размерах. Код проходит clang-tidy и рассчитан на кроссплатформенную CI-среду с санитайзерами.

## 9. Источники
1. Документация курса «Параллельное программирование» — https://learning-process.github.io/parallel_programming_course/ru/index.html
2. Cppreference: ranges — https://en.cppreference.com/w/cpp/ranges
3. Open MPI Documentation — https://www.open-mpi.org/doc/