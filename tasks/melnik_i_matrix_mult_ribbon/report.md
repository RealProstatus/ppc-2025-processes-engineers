# Ленточная горизонтальная схема - разбиение только матрицы А - умножение матрицы на матрицу

- Студент: Мельник Иван Вадимович, группа 3823Б1ПР1
- Технологии: SEQ, MPI
- Вариант: 13

## 1. Введение
В работе рассмотрена задача умножения матрицы на матрицу с ленточным горизонтальным разбиением матрицы `A` между MPI-процессами. Цель работы — разработать последовательную и MPI-параллельную версии алгоритма, сравнить их производительность и проанализировать влияние коммуникаций на масштабируемость.

## 2. Постановка задачи
**Формальная постановка**\
Даны матрицы `A (m × n)` и `B (n × k)`. Требуется вычислить `C = A * B`.\
**Дополнительные требования и ограничения**
- Алгоритмы должны корректно работать на любых размерах матрицы;
- Параллельная версия должна быть реализована с помощью MPI и быть стабильной на любом разумном количестве процессов;
- Между процессами распределяется только матрица `A`, а точнее, ее строки. Матрица `B` видна всем процессам; 
- Поддержка неделимости строк матрицы `A` на число процессов: строки распределяются с остатком (процесс может и не получить данных матрицы `A`)

## 3. Базовый алгоритм (последовательная версия)
**Принцип работы**\
Последовательный алгоритм реализует классическое умножение матриц через три вложенных цикла. Каждый элемент результирующей матрицы `C[i][j]` вычисляется как скалярное произведение строки `A[i]` на столбец `B[:,j]`, то есть суммирование произведений `A[i][k] * B[k][j]` по всем значениям `k` от 0 до `n-1`.

**Структура циклов**\
Внешний цикл перебирает строки матрицы `A` (индекс `i`), средний цикл — элементы строки `A[i]` (индекс `k`), внутренний цикл — столбцы матрицы `B` (индекс `j`).

**Реализация**\
В коде значение `A[i][k]` выносится в переменную `aik` перед внутренним циклом, что исключает повторные обращения к памяти. Затем для каждого столбца `j` матрицы `B` выполняется накопление произведения `aik * B[k][j]` в элемент `C[i][j]`.

**Вычислительная сложность**\
Алгоритм выполняет `m × n × k` операций умножения и столько же операций сложения, что даёт временную сложность `O(mnk)`. Для квадратных матриц `n × n` сложность составляет `O(n³)`. Пространственная сложность — `O(n²)` для квадратных матриц, так как требуется хранить три матрицы одинакового размера.

## 4. Схема распараллеливания
### 4.1. Общая идея
Матрица `A` разбивается по строкам между процессами, а матрица `B` полностью реплицируется на всех процессах через широковещательную рассылку `MPI_Bcast`. Каждый процесс получает свой блок строк матрицы `A` и вычисляет соответствующие строки результирующей матрицы `C`, затем возвращает результат для последующей сборки

**Транспонирование матрицы B**\
На этапе предобработки матрица `B` транспонируется на корневом процессе и сохраняется в векторе `flat_b_transposed_`. Транспонирование позволяет обращаться к столбцам исходной матрицы `B` как к строкам транспонированной матрицы, что улучшает производительность. Матрицы `A` и `C` также сохраняются в векторе. 

### 4.2. Распределение данных
**Распределение строк матрицы A**\
Строки матрицы `A` распределяются между процессами с учётом остатка от деления. Базовое количество строк на процесс вычисляется как `base = rows_a / proc_num`, остаток — как `remainder = rows_a % proc_num`. Первые `remainder` процессов получают `base + 1` строк, остальные — `base` строк. В случае, когда `rows_a < proc_num`, часть процессов получает 0 строк, но корректно завершает выполнение без ошибок.

**Использование MPI_Scatterv**\
Для неравномерного распределения используется `MPI_Scatterv`, которая требует массивов `counts` и `displs`. Массив `counts[i]` содержит количество элементов для процесса `i` (равно `rows_per_rank[i] * cols_a`), массив `displs[i]` — смещение в исходном буфере. Матрица `A` предварительно преобразуется в плоский вектор `flat_a_` путём последовательного размещения всех строк.

### 4.3. Локальные вычисления
**Алгоритм умножения**\
Каждый процесс умножает свой блок строк матрицы `A` на транспонированную матрицу `B^T`. Для каждой локальной строки `A[i]` вычисляются все элементы строки `C[i]` путём скалярного умножения строки `A[i]` на каждый столбец матрицы `B` ( равно строка в `B^T`).

**Реализация через std::transform_reduce**\
Вычисление каждого элемента `C[i][j]` выполняется с помощью `std::transform_reduce`, который за один проход вычисляет скалярное произведение двух последовательностей. Алгоритм применяет операцию умножения к парам элементов из строки `A[i]` и строки `B^T[j]`, затем накапливает результат через операцию сложения. Это позволяет эффективно использовать векторизацию и конвейеризацию процессора.

### 4.4. Сбор результатов
**Сбор через MPI_Gatherv**\
После локальных вычислений каждый процесс имеет фрагмент матрицы `C` в буфере `local_c`. Для формирования полной матрицы используется `MPI_Gatherv`, которая собирает локальные результаты в буфер `flat_c_` на корневом процессе. Массивы `counts` и `displs` для сбора вычисляются аналогично распределению: `counts[i] = rows_per_rank[i] * cols_b`, смещения рассчитываются с учётом количества элементов, собранных предыдущими процессами.

**Рассылка финального результата**\
После сборки полной матрицы `C` на корневом процессе выполняется `MPI_Bcast`, который рассылает результат всем процессам.

### 4.5. Синхронизация процессов
Все процессы равноправны в выполнении вычислений, но корневой процесс дополнительно выполняет инициализацию, валидацию и рассылку данных, агрегацию результатов.

## 5. Детали реализации
### 5.1. Структура реализации
```text
melnik_i_matrix_mult_ribbon
│   settings.json
│
├───common
│   └───include
│           common.hpp - определение типов входных/выходных/тестовых данных
├───mpi   
│   ├───include
│   |       ops_mpi.hpp - заголовочный файл MPI-реализации
│   │
│   └───src
│           ops_mpi.cpp - код MPI-реализации
├───seq
│   ├───include
│   │       ops_seq.hpp - заголовочный файл SEQ-реализации
│   │
│   └───src
│           ops_seq.cpp - код SEQ-реализации
└───tests
    ├───functional
    │       main.cpp - функциональные тесты
    │
    └───performance
            main.cpp - тесты производительности
```
### 5.2. Основные классы / функции
- `MelnikIMatrixMultRibbonSEQ` — последовательное умножение матриц.
- `MelnikIMatrixMultRibbonMPI`:
  - `ValidateOnRoot` — валидация входных данных на нулевом процессе;
  - `HasUniformRowWidth` — проверка одинаковой ширины строк;
  - `ShareSizes` — рассылка размеров матриц;
  - `ShareMatrixB` — рассылка транспонированной матрицы B;
  - `ScatterRows` — распределение строк A с остатком;
  - `MultiplyLocal` — локальное умножение;
  - `GatherAndDistribute` — сбор и рассылка результата.
### 5.3. Обработка граничных случаев и замечания
- Процессы, получающие 0 строк, проходят все стадии без вычислений.
- Валидация отклоняет рваные матрицы и несовпадающие размеры.
- Постобработка преобразует плоский вектор результата `C` в вектор векторов.
### 5.4. Пространственная и временная сложности алгоритмов 
Расчеты приведены для квадратных входных матриц со стороной `n`: 
- SEQ: `O(n^3)` по времени, `O(n^2)` по памяти.
- MPI: примерно `O((n^3)/p + n^2)` (умножение + широковещание/сбор), память — `O(n^2/p)` на рабочих рангах и `O(n^2)` на root.

## 6. Тестовая инфраструктура
### 6.1. Аппаратное обеспечение
| Параметр | Значение                                                                               |
| -------- | -------------------------------------------------------------------------------------- |
| CPU      | Intel Core i5-11400F (6 cores, 12 threads, 2.6 GHz, up to 4.40 Ghz, L3 Cache 12 MB)    |
| RAM      | 32 GB DDR4 (3200 MHz)                                                                  |
### 6.2. Программное обеспечение:
| Параметр   | Значение                                               |
| ---------- | ------------------------------------------------------ |
| ОС         | Windows 10 Pro 22H2 (19045.5965) + WSL (Ubuntu 13.3.0) |
| MPI        | OpenMPI 3.1                                            |
| Компилятор | g++ 14.2.0                                             |
| Сборка     | Release                                                |
### 6.3. Тестовые данные
- **Функциональные тесты:**
  - Единичные, нулевые, диагональные (в т.ч. с крупными и отрицательными значениями);
  - Полностью единичные матрицы;
  - Инкрементные строки * I (проверка тождественности);
  - Альтернирующие знаки * диагональ;
  - Векторные случаи 1×3·3×1 и скаляр 1×1.
- **Perf-тест:** умножение плотных матриц 1024x1024, рандомная генерация значений 

## 7. Результаты и обсуждение
### 7.1. Корректность
Функциональные тесты охватывают разнообразные граничные и типовые сценарии. SEQ и MPI выдают идентичные результаты на наборах 1/2/4/6 процессов, что подтверждает корректность распределения с остатком и сборки.
### 7.2. Производительность
**Метрики:**
1. Абсолютное время выполнения вычислительной части алгоритма в миллисекундах;
2. Ускорение относительно последовательной версии;
3. Эффективность распараллеливания = `(ускорение / число процессов) * 100%`.

**Размер матриц = 1024х1024**

| Режим | Процессы | Время, мс | Ускорение | Эффективность |
| ----- | -------- | --------- | --------- | ------------- |
| seq   | 1        | 2858      | 1.000     | N/A           |
| mpi   | 2        | 2006      | 1.424     | 71.23%        |
| mpi   | 4        | 1383      | 2.066     | 51.66%        |
| mpi   | 7        | 1155      | 2.474     | 35.34%        |
| mpi   | 8        | 1068      | 2.676     | 33.45%        |

**Размер матриц = 2048х2048**

| Режим | Процессы | Время, мс  | Ускорение | Эффективность |
| ----- | -------- | ---------- | --------- | ------------- |
| seq   | 1        | 20206      | 1.000     | N/A           |
| mpi   | 2        | 13010      | 1.553     | 77.65%        |
| mpi   | 4        | 10076      | 2.005     | 50.13%        |
| mpi   | 7        | 8637       | 2.339     | 33.42%        |
| mpi   | 8        | 8184       | 2.469     | 30.86%        |

**Анализ полученных результатов:**

На основе полученных данных можно выделить следующие закономерности:

Нелинейный рост производительности объясняется накладными расходами на распараллеливание.\
Для больших матриц (2048×2048) эффективность при 2 процессах выше (77.65% против 71.23%), что подтверждает улучшение отношения вычислений к коммуникациям. Однако при 4 процессах разница минимальна (50.13% против 51.66%), что указывает на сохраняющееся доминирование коммуникационных затрат при большом числе процессов.\
Переход на 7, 8 процессов (больше кол-ва физических ядер тестовой системы) дает небольшое улучшение времени, однако эффективность снижается вследствие еще больших наклданых расходов на паралелезацию.
Как следствие - при заданной размерности необходимо выбирать оптимальное количество процессов, так как дальнейшее увеличение числа процессов дает все меньшую прибавку к эффективности.


## 8. Выводы
Получены устойчивые SEQ и MPI реализации ленточного умножения квадратных матриц.  Тестовый набор покрывает граничные и векторные случаи, а perf-тест подтверждает работоспособность на средних размерах. Тестирование производительности показывает, что параллелизация полезна для данной задачи.

## 9. Источники
1. Документация по курсу «Параллельное программирование» // Parallel Programming Course URL: https://learning-process.github.io/parallel_programming_course/ru/index.html (дата обращения: 10.11.2025).
2. Ranges library // Cppreference URL: https://en.cppreference.com/w/cpp/ranges.html (дата обращения: 12.11.2025).
3. Ленточное умножение матриц (MPI) // Центр суперкомпьютерных технологий URL: http://www.hpcc.unn.ru/?dir=808 (дата обращения: 16.12.2025).
4. Open MPI v3.1.6 documentation // Open MPI URL: https://www.open-mpi.org/doc/v3.1/ (дата обращения: 12.11.2025).
5. Intel® Core™ i5-11400F Processor // Intel® Products URL: https://www.intel.com/content/www/us/en/products/sku/212271/intel-core-i511400f-processor-12m-cache-up-to-4-40-ghz/specifications.html (дата обращения: 18.11.2025).